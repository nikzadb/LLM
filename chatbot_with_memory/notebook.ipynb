{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b5280b",
   "metadata": {},
   "source": [
    "# Building a Custom Chatbot with Memory: A Complete Guide using LangChain and Gemini\n",
    "\n",
    "## Introduction\n",
    "In today's AI landscape, creating chatbots that can maintain context and remember past conversations is crucial for delivering personalised, engaging user experiences. In this tutorial, we'll build a chatbot that not only responds intelligently but also remembers previous interactions, making conversations feel more natural and contextual.\n",
    "We'll leverage the power of Google's Gemini model through LangChain, implement both short-term and long-term memory using vector databases, and create a robust system that can be easily extended for various applications.\n",
    "\n",
    "## Why Memory Matters in Chatbots\n",
    "Traditional chatbots treat each interaction as isolated, leading to frustrating experiences where users must repeat information. A memory-enabled chatbot can:\n",
    "* Remember user preferences and personal details\n",
    "* Maintain conversation context across sessions\n",
    "* Provide more accurate and personalised responses\n",
    "* Create a more human-like interaction experience\n",
    "* Build rapport with users over time\n",
    "* Learn from past interactions to improve future responses\n",
    "\n",
    "## What You'll Learn\n",
    "* Prompt Engineering: Crafting effective prompts for consistent, contextual responses\n",
    "* Memory Systems: Implementing both short-term and long-term memory\n",
    "* Vector Databases: Using ChromaDB for efficient semantic search\n",
    "* Context Management: Maintaining conversation flow across interactions\n",
    "* RAG Implementation: Building a basic Retrieval Augmented Generation system\n",
    "* Error Handling: Creating robust error management and recovery mechanisms\n",
    "\n",
    "## Prerequisites\n",
    "Before we dive in, make sure you have:\n",
    "* Python 3.10 or higher\n",
    "* A Google Cloud account with Gemini API access\n",
    "* Basic understanding of Python and APIs\n",
    "* Familiarity with LangChain (optional but helpful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install langchain \\\n",
    "#              google.generativeai \\\n",
    "#              langchain-google-genai \\\n",
    "#              langchain-community \\\n",
    "#              chromadb \\\n",
    "#              pytest \\\n",
    "#              \"langchain-chroma>=0.1.2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f635d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e7ece87",
   "metadata": {},
   "source": [
    "## Project Architecture\n",
    "Our chatbot architecture consists of four main components:\n",
    "* __LLM Interface:__ Google's Gemini model via LangChain for natural language processing\n",
    "* __Short-term Memory:__ ConversationBufferMemory for immediate context within a session\n",
    "* __Long-term Memory:__ ChromaDB vector store for persistent storage across sessions\n",
    "* __Memory Retrieval:__ Semantic search for finding relevant past conversations\n",
    "\n",
    "```python\n",
    "+-------------------+     +------------------+     +------------------+\n",
    "|   User Input      | --> | Memory Retrieval | --> | Gemini LLM       |\n",
    "+-------------------+     +------------------+     +------------------+\n",
    "                                   ^                        |\n",
    "                                   |                        v\n",
    "                          +------------------+     +------------------+\n",
    "                          | Vector Database  | <-- | Response + Memory|\n",
    "                          +------------------+     +------------------+\n",
    "```                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361943e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33cb4af2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26af1041",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up the Environment\n",
    "\n",
    "First, let's set up our development environment and configure the necessary components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e98a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file (if available)\n",
    "load_dotenv('.env')\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5d960",
   "metadata": {},
   "source": [
    "## Step 2: Creating a Configuration System\n",
    "\n",
    "To make our chatbot flexible and maintainable, we'll implement a configuration system using dataclasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChatbotConfig:\n",
    "    \"\"\"Configuration for the chatbot.\"\"\"\n",
    "\n",
    "    google_api_key: str\n",
    "    model_name: str = 'gemini-2.0-flash'\n",
    "    temperature: float = 0.7\n",
    "    embedding_model: str = 'models/gemini-embedding-exp-03-07'\n",
    "    chroma_persist_dir: str = './chroma_langchain_db'\n",
    "    memory_k: int = 3 # Number of relevant memories to retrieve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cf507",
   "metadata": {},
   "source": [
    "## Step 3: Building the Memory Management System\n",
    "\n",
    "The heart of our chatbot is its dual-memory architecture. Let's implement a sophisticated memory system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11418f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotMemory:\n",
    "    \"\"\"Memory management for the chatbot.\"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, vector_store: Chroma, memory_k: int = 3):\n",
    "        \"\"\"Initilise the memory system.\"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embeddings = embeddings\n",
    "        self.memory_k = memory_k\n",
    "        self.short_term_memory = ConversationBufferMemory()\n",
    "\n",
    "        # Track conversation metadata\n",
    "        self.conversation_count = 0 \n",
    "        self.last_interaction_time = None\n",
    "\n",
    "    def add_to_memory(self, human_input: str, ai_response: str) -> None:\n",
    "        \"\"\"Add conversation to both short-term and long-term memory\"\"\"\n",
    "\n",
    "        # Update metadata\n",
    "        self.conversation_count += 1\n",
    "        self.last_interaction_time = datetime.now()\n",
    "        metadata = {\n",
    "            \"timestamp\": self.last_interaction_time.isoformat(),\n",
    "            \"conversation_id\": self.conversation_count\n",
    "        }        \n",
    "\n",
    "        # Short-term memory\n",
    "        self.short_term_memory.save_context(\n",
    "            {\"input\": human_input},\n",
    "            {\"output\": ai_response}\n",
    "        )\n",
    "        \n",
    "        # long-term memory\n",
    "        memory_text = f\"Human: {human_input}\\nAi: {ai_response}\"\n",
    "\n",
    "        self.vector_store.add_texts(\n",
    "            texts = [memory_text],\n",
    "            metadata = [metadata]\n",
    "        )\n",
    "\n",
    "    def get_relevant_memories(self, query: str) -> str:\n",
    "        \"\"\"Retrieve relevant past conversations.\"\"\"\n",
    "\n",
    "        docs = self.vector_store.similarity_search(query, k=self.memory_k)\n",
    "\n",
    "        formated_memories = []\n",
    "        for doc in docs:\n",
    "            metadata = doc.metadata\n",
    "            timestamp = metadata.get(\"timestamp\", \"Unknown time\")\n",
    "            formated_memories.append(f\"[{timestamp}]\\n{doc.page_content}\")\n",
    "\n",
    "        return \"\\n\\n\".join(formated_memories)\n",
    "\n",
    "    def get_conversation_history(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the recent conversation history.\"\"\"\n",
    "        return self.short_term_memory.load_memory_variables({})\n",
    "    \n",
    "    def clear_short_term_memory(self) -> None:\n",
    "        \"\"\"Clear the short-term memory.\"\"\"\n",
    "        self.short_term_memory.clear()\n",
    "    \n",
    "    def get_all_memories(self) -> List[Document]:\n",
    "        \"\"\"Get all memories from the vector store.\"\"\"\n",
    "        # This is a simplified version - in practice, you'd implement pagination\n",
    "        return self.vector_store.similarity_search(\"\", k=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af20e8b",
   "metadata": {},
   "source": [
    "## Step 4: Creating the Enhanced Chatbot\n",
    "Now let's build our main chatbot class that brings everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c073b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedChatbot:\n",
    "    \"\"\"Chatbot with enhanced memory capabilities.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[ChatbotConfig] = None):\n",
    "        \"\"\"Initialise the chatbot.\"\"\"\n",
    "        self.config = config or ChatbotConfig.from_env()\n",
    "        self._setup_components()\n",
    "\n",
    "    @staticmethod\n",
    "    def format_conversation_history(history: Dict[str, Any]) -> str:\n",
    "        \"\"\"Format conversation history for display.\"\"\"\n",
    "        return history.get('history', '')\n",
    "\n",
    "    def _setup_components(self) -> None:\n",
    "        \"\"\"Set up chatbot components.\"\"\"\n",
    "\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=self.config.model_name,\n",
    "            temperature=self.config.temperature,\n",
    "            google_api_key=self.config.google_api_key\n",
    "        )\n",
    "\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=self.config.embedding_model\n",
    "        )\n",
    "\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=\"conversation_memory\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=self.config.chroma_persist_dir\n",
    "        )\n",
    "\n",
    "        self.memory = ChatbotMemory(\n",
    "            vector_store=self.vector_store,\n",
    "            embeddings=self.embeddings,\n",
    "            memory_k=self.config.memory_k\n",
    "        )\n",
    "\n",
    "        self.prompt_template = self._create_prompt_template()\n",
    "\n",
    "    def _create_prompt_template(self) -> PromptTemplate:\n",
    "        \"\"\"Create the prompt template for the chatbot.\"\"\"\n",
    "        template = \"\"\"You are a helpful AI assistant with memory of past converstaions.\n",
    "        \n",
    "        Relevant past conversations:\n",
    "        {relevant_memories}\n",
    "\n",
    "        Recent conversation:\n",
    "        {recent_history}\n",
    "\n",
    "        Human: {input}\n",
    "        AI Assistant:\n",
    "        \"\"\"\n",
    "\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"relevant_memories\", \"recent_history\", \"input\"],\n",
    "            template=template\n",
    "        )\n",
    "\n",
    "    def generate_response(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a response to user input.\"\"\"\n",
    "        try:\n",
    "            relevant_memories = self.memory.get_relevant_memories(user_input)\n",
    "\n",
    "            recent_history = self.__class__.format_conversation_history(\n",
    "                self.memory.get_conversation_history()\n",
    "            )\n",
    "\n",
    "            prompt = self.prompt_template.format(\n",
    "                relevant_memories=relevant_memories,\n",
    "                recent_history=recent_history,\n",
    "                input=user_input\n",
    "            )\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.predict(prompt)\n",
    "\n",
    "            # Add to memory\n",
    "            self.memory.add_to_memory(user_input, response)\n",
    "\n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"relevant_memories\": relevant_memories,\n",
    "                \"success\": True\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response; {e}\")\n",
    "            return {\n",
    "                \"response\": \"I'm sorry, I encountered an error. Please try again.\",\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            }\n",
    "        \n",
    "    def clear_memory(self) -> None:\n",
    "        \"\"\"Clear the chatbot's memory.\"\"\"\n",
    "        self.memory.clear_short_term_memory()\n",
    "        logger.info(\"Short-term memory cleared\")\n",
    "\n",
    "    def get_conversation_history(self) -> str:\n",
    "        \"\"\"Get the formated conversation history.\"\"\"\n",
    "        return self.__class__.format_conversation_history(\n",
    "            self.memory.get_conversation_history\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e7142",
   "metadata": {},
   "source": [
    "## Interactive Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fcc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interface(chatbot: EnhancedChatbot):\n",
    "    \"\"\"Simple interactive chat interface.\"\"\"\n",
    "    print(\"Enhanced Chatbot with Memory\")\n",
    "    print(\"Type 'exit' to quit, 'clear' to clear memory\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_memory()\n",
    "            print(\"Memory cleared!\")\n",
    "            continue\n",
    "        \n",
    "        response = chatbot.generate_response(user_input)\n",
    "        print(f\"Bot: {response['response']}\")\n",
    "        \n",
    "        if not response['success']:\n",
    "            print(f\"Error: {response.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Uncomment to run the interactive interface\n",
    "config = ChatbotConfig(google_api_key=GOOGLE_API_KEY)\n",
    "chatbot = EnhancedChatbot(config)\n",
    "chat_interface(chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239e79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
