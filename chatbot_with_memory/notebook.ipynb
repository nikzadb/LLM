{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b5280b",
   "metadata": {},
   "source": [
    "# Building a Custom Chatbot with Memory: A Complete Guide using LangChain and Gemini\n",
    "\n",
    "## Introduction\n",
    "In today's AI landscape, creating chatbots that can maintain context and remember past conversations is crucial for delivering personalised, engaging user experiences. In this tutorial, we'll build a chatbot that not only responds intelligently but also remembers previous interactions, making conversations feel more natural and contextual.\n",
    "We'll leverage the power of Google's Gemini model through LangChain, implement both short-term and long-term memory using vector databases, and create a robust system that can be easily extended for various applications.\n",
    "\n",
    "## Why Memory Matters in Chatbots\n",
    "Traditional chatbots treat each interaction as isolated, leading to frustrating experiences where users must repeat information. A memory-enabled chatbot can:\n",
    "* Remember user preferences and personal details\n",
    "* Maintain conversation context across sessions\n",
    "* Provide more accurate and personalised responses\n",
    "* Create a more human-like interaction experience\n",
    "* Build rapport with users over time\n",
    "* Learn from past interactions to improve future responses\n",
    "\n",
    "## What You'll Learn\n",
    "* Prompt Engineering: Crafting effective prompts for consistent, contextual responses\n",
    "* Memory Systems: Implementing both short-term and long-term memory\n",
    "* Vector Databases: Using ChromaDB for efficient semantic search\n",
    "* Context Management: Maintaining conversation flow across interactions\n",
    "* RAG Implementation: Building a basic Retrieval Augmented Generation system\n",
    "* Error Handling: Creating robust error management and recovery mechanisms\n",
    "\n",
    "## Prerequisites\n",
    "Before we dive in, make sure you have:\n",
    "* Python 3.10 or higher\n",
    "* A Google Cloud account with Gemini API access\n",
    "* Basic understanding of Python and APIs\n",
    "* Familiarity with LangChain (optional but helpful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01f0d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install langchain \\\n",
    "#              google.generativeai \\\n",
    "#              langchain-google-genai \\\n",
    "#              langchain-community \\\n",
    "#              chromadb \\\n",
    "#              pytest \\\n",
    "#              \"langchain-chroma>=0.1.2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f635d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e7ece87",
   "metadata": {},
   "source": [
    "## Project Architecture\n",
    "Our chatbot architecture consists of four main components:\n",
    "* __LLM Interface:__ Google's Gemini model via LangChain for natural language processing\n",
    "* __Short-term Memory:__ ConversationBufferMemory for immediate context within a session\n",
    "* __Long-term Memory:__ ChromaDB vector store for persistent storage across sessions\n",
    "\n",
    "```python\n",
    "+-------------------+     +------------------+     +------------------+\n",
    "|   User Input      | --> | Memory Retrieval | --> | Gemini LLM       |\n",
    "+-------------------+     +------------------+     +------------------+\n",
    "                                   ^                        |\n",
    "                                   |                        v\n",
    "                          +------------------+     +------------------+\n",
    "                          | Vector Database  | <-- | Response + Memory|\n",
    "                          +------------------+     +------------------+\n",
    "```                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361943e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33cb4af2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26af1041",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up the Environment\n",
    "\n",
    "First, let's set up our development environment and configure the necessary components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e98a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file (if available)\n",
    "load_dotenv('.env')\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5d960",
   "metadata": {},
   "source": [
    "## Step 2: Creating a Configuration System\n",
    "\n",
    "To make our chatbot flexible and maintainable, we'll implement a configuration system using dataclasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bbb8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChatbotConfig:\n",
    "\n",
    "    google_api_key: str\n",
    "    model_name: str = 'gemini-2.0-flash'\n",
    "    temperature: float = 0.7\n",
    "    embedding_model: str = 'models/gemini-embedding-exp-03-07'\n",
    "    chroma_persist_dir: str = './chroma_langchain_db'\n",
    "    memory_k: int = 3 # Number of relevant memories to retrieve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cf507",
   "metadata": {},
   "source": [
    "## Step 3: Building the Memory Management System\n",
    "\n",
    "The heart of our chatbot is its dual-memory architecture. Let's implement a sophisticated memory system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11418f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotMemory:\n",
    "    \"\"\"Memory management for the chatbot.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: Chroma, memory_k: int = 3):\n",
    "        \n",
    "        self.vector_store = vector_store\n",
    "        self.memory_k = memory_k\n",
    "        self.short_term_memory = ConversationBufferMemory()\n",
    "\n",
    "\n",
    "    def add_to_memory(self, human_input: str, ai_response: str) -> None:\n",
    "        \"\"\"Add conversation to both short-term and long-term memory\"\"\"\n",
    "\n",
    "        # Short-term memory\n",
    "        self.short_term_memory.save_context(\n",
    "            {\"input\": human_input},\n",
    "            {\"output\": ai_response}\n",
    "        )\n",
    "\n",
    "        # long-term memory\n",
    "        metadata = {\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }   \n",
    "\n",
    "        memory_text = f\"Human: {human_input}\\nAi: {ai_response}\"\n",
    "\n",
    "        self.vector_store.add_texts(\n",
    "            texts = [memory_text],\n",
    "            metadata = [metadata]\n",
    "        )\n",
    "\n",
    "    def get_relevant_memories(self, query: str) -> str:\n",
    "        \"\"\"Retrieve similar past conversations from vector database.\"\"\"\n",
    "\n",
    "        docs = self.vector_store.similarity_search(query, k=self.memory_k)\n",
    "\n",
    "        formated_memories = []\n",
    "        for doc in docs:\n",
    "            metadata = doc.metadata\n",
    "            timestamp = metadata.get(\"timestamp\", \"Unknown time\")\n",
    "            formated_memories.append(f\"{timestamp}: {doc.page_content}\")\n",
    "\n",
    "        return \"\\n\\n\".join(formated_memories)\n",
    "\n",
    "    def get_conversation_history(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the recent conversation history in the session.\"\"\"\n",
    "        return self.short_term_memory.load_memory_variables({})\n",
    "    \n",
    "    def clear_short_term_memory(self) -> None:\n",
    "        \"\"\"Clear the short-term memory.\"\"\"\n",
    "        self.short_term_memory.clear()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af20e8b",
   "metadata": {},
   "source": [
    "## Step 4: Creating the Enhanced Chatbot\n",
    "Now let's build our main chatbot class that brings everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84c073b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedChatbot:\n",
    "    \"\"\"Chatbot with short-term and long-term memories.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[ChatbotConfig] = None):\n",
    "\n",
    "        self.config = config or ChatbotConfig.from_env()\n",
    "\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=self.config.model_name,\n",
    "            temperature=self.config.temperature,\n",
    "            google_api_key=self.config.google_api_key\n",
    "        )\n",
    "\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=self.config.embedding_model\n",
    "        )\n",
    "\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=\"conversation_memory\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=self.config.chroma_persist_dir\n",
    "        )\n",
    "\n",
    "        self.memory = ChatbotMemory(\n",
    "            vector_store=self.vector_store,\n",
    "            memory_k=self.config.memory_k\n",
    "        )\n",
    "\n",
    "        self.prompt_template = self._create_prompt_template()        \n",
    "\n",
    "    def _create_prompt_template(self) -> PromptTemplate:\n",
    "        \"\"\"Create the prompt template for the chatbot.\"\"\"\n",
    "\n",
    "        template = \"\"\"You are a helpful AI assistant with memory of past converstaions.\n",
    "        \n",
    "        Relevant past conversations:\n",
    "        {relevant_memories}\n",
    "\n",
    "        Recent conversation:\n",
    "        {recent_history}\n",
    "\n",
    "        Human: {user_input}\n",
    "        AI Assistant:\n",
    "        \"\"\"\n",
    "\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"relevant_memories\", \"recent_history\", \"user_input\"],\n",
    "            template=template\n",
    "        )\n",
    "\n",
    "    def generate_response(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a response to user input.\"\"\"\n",
    "\n",
    "        try:\n",
    "            relevant_memories = self.memory.get_relevant_memories(user_input)\n",
    "\n",
    "            recent_session_history = self.memory.get_conversation_history().get('history', '')\n",
    "\n",
    "            prompt = self.prompt_template.format(\n",
    "                relevant_memories=relevant_memories,\n",
    "                recent_history=recent_session_history,\n",
    "                user_input=user_input\n",
    "            )\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.predict(prompt)\n",
    "\n",
    "            # Add to memory\n",
    "            self.memory.add_to_memory(user_input, response)\n",
    "\n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"relevant_memories\": relevant_memories,\n",
    "                \"success\": True\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response; {e}\")\n",
    "            return {\n",
    "                \"response\": \"I'm sorry, I encountered an error. Please try again.\",\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            }\n",
    "        \n",
    "    def clear_memory(self) -> None:\n",
    "        \"\"\"Clear the chatbot's memory.\"\"\"\n",
    "        self.memory.clear_short_term_memory()\n",
    "        logger.info(\"Short-term memory cleared\")\n",
    "\n",
    "    def get_conversation_history(self) -> str:\n",
    "        \"\"\"Get the formated conversation history.\"\"\"\n",
    "        return self.memory.get_conversation_history.get('history', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e7142",
   "metadata": {},
   "source": [
    "## Interactive Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c30fcc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/w0k8k7s135vc0mrp37stcbhc0000gn/T/ipykernel_1134/2795249757.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.short_term_memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Chatbot with Memory\n",
      "Type 'exit' to quit, 'clear' to clear memory\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/w0k8k7s135vc0mrp37stcbhc0000gn/T/ipykernel_1134/4247017824.py:66: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I do not have a name. You can call me AI Assistant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Please provide more context or clarify your request. I only see the letter \"p\".  What would you like me to do with it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Please provide more context or clarify your request. I only see the letter \"p\". What would you like me to do with it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error generating response; Error embedding content: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm sorry, I encountered an error. Please try again.\n",
      "Error: Error embedding content: 429 Resource has been exhausted (e.g. check quota).\n",
      "Bot: I'm not sure what you're asking. Could you please rephrase your question or provide more information?\n",
      "Bot: I'm sorry, I do not have real-time access to weather information for New York. To find out the current weather conditions, I recommend checking a reliable weather app or website.\n",
      "Bot: The next FIFA World Cup will be in 2026.\n",
      "Bot: The 2026 FIFA World Cup will be held in the United States, Canada, and Mexico. There are multiple cities across these three countries that will host matches.\n",
      "Bot: While Australia has not been selected as a host for the 2026 FIFA World Cup, there's always a possibility in future tournaments. FIFA regularly evaluates bids from different countries.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def chat_interface(chatbot: EnhancedChatbot):\n",
    "    \"\"\"Simple interactive chat interface.\"\"\"\n",
    "    print(\"Enhanced Chatbot with Memory\")\n",
    "    print(\"Type 'exit' to quit, 'clear' to clear memory\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_memory()\n",
    "            print(\"Memory cleared!\")\n",
    "            continue\n",
    "        \n",
    "        response = chatbot.generate_response(user_input)\n",
    "        print(f\"Bot: {response['response']}\")\n",
    "        \n",
    "        if not response['success']:\n",
    "            print(f\"Error: {response.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Uncomment to run the interactive interface\n",
    "config = ChatbotConfig(google_api_key=GOOGLE_API_KEY)\n",
    "chatbot = EnhancedChatbot(config)\n",
    "chat_interface(chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239e79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
