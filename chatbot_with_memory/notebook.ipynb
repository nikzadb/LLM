{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b5280b",
   "metadata": {},
   "source": [
    "# Building a Custom Chatbot with Memory: A Complete Guide using LangChain and Gemini\n",
    "\n",
    "## Introduction\n",
    "In today's AI landscape, creating chatbots that can maintain context and remember past conversations is crucial for delivering personalised, engaging user experiences. In this tutorial, we'll build a chatbot that not only responds intelligently but also remembers previous interactions, making conversations feel more natural and contextual.\n",
    "We'll leverage the power of Google's Gemini model through LangChain, implement both short-term and long-term memory using vector databases, and create a robust system that can be easily extended for various applications.\n",
    "\n",
    "## Why Memory Matters in Chatbots\n",
    "Traditional chatbots treat each interaction as isolated, leading to frustrating experiences where users must repeat information. A memory-enabled chatbot can:\n",
    "* Remember user preferences and personal details\n",
    "* Maintain conversation context across sessions\n",
    "* Provide more accurate and personalised responses\n",
    "* Create a more human-like interaction experience\n",
    "* Build rapport with users over time\n",
    "* Learn from past interactions to improve future responses\n",
    "\n",
    "## What You'll Learn\n",
    "* Prompt Engineering: Crafting effective prompts for consistent, contextual responses\n",
    "* Memory Systems: Implementing both short-term and long-term memory\n",
    "* Vector Databases: Using ChromaDB for efficient semantic search\n",
    "* Context Management: Maintaining conversation flow across interactions\n",
    "* Error Handling: Creating robust error management and recovery mechanisms\n",
    "\n",
    "## Prerequisites\n",
    "Before we dive in, make sure you have:\n",
    "* Python 3.10 or higher\n",
    "* A Google Cloud account with Gemini API access\n",
    "* Basic understanding of Python and APIs\n",
    "* Familiarity with LangChain (optional but helpful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ece87",
   "metadata": {},
   "source": [
    "## Project Architecture\n",
    "Our chatbot architecture consists of four main components:\n",
    "* __LLM Interface:__ Google's Gemini model via LangChain for natural language processing\n",
    "* __Short-term Memory:__ ConversationBufferMemory for immediate context within a session\n",
    "* __Long-term Memory:__ ChromaDB vector store for persistent storage across sessions\n",
    "* __Memory Retrieval:__ Semantic search for finding relevant past conversations\n",
    "\n",
    "```python\n",
    "+-------------------+     +------------------+     +------------------+\n",
    "|   User Input      | --> | Memory Retrieval | --> | Gemini LLM       |\n",
    "+-------------------+     +------------------+     +------------------+\n",
    "                                   ^                        |\n",
    "                                   |                        v\n",
    "                          +------------------+     +------------------+\n",
    "                          | Vector Database  | <-- | Response + Memory|\n",
    "                          +------------------+     +------------------+\n",
    "```                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af1041",
   "metadata": {},
   "source": [
    "## Step 1: Install Necessary Packages\n",
    "\n",
    "First, let's set up our development environment and configure the necessary components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842da91",
   "metadata": {
    "vscode": {
     "languageId": "dockerfile"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain \\\n",
    "             google.generativeai \\\n",
    "             langchain-google-genai \\\n",
    "             langchain-community \\\n",
    "             chromadb \\\n",
    "             pytest \\\n",
    "             \"langchain-chroma>=0.1.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e98a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikzadbabaii/Desktop/projects/learning/LLM/chatbot_with_memory/.venv_chatbot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file (if available)\n",
    "load_dotenv('.env')\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5d960",
   "metadata": {},
   "source": [
    "## Step 2: Creating a Configuration System\n",
    "\n",
    "To make our chatbot flexible and maintainable, we'll implement a configuration system using dataclasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbb8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ChatbotConfig:\n",
    "\n",
    "    google_api_key: str\n",
    "    model_name: str = 'gemini-2.0-flash'\n",
    "    temperature: float = 0.7\n",
    "    embedding_model: str = 'models/gemini-embedding-exp-03-07'\n",
    "    chroma_persist_dir: str = './chroma_langchain_db'\n",
    "    memory_k: int = 3 # Number of relevant memories to retrieve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cf507",
   "metadata": {},
   "source": [
    "## Step 3: Building the Memory Management System\n",
    "\n",
    "The heart of our chatbot is its dual-memory architecture. Let's implement the memory system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11418f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ChatbotMemory:\n",
    "    \"\"\"Memory management for the chatbot.\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: Chroma, memory_k: int = 3):\n",
    "\n",
    "        self.vector_store = vector_store\n",
    "        self.memory_k = memory_k\n",
    "        self.short_term_memory = ConversationBufferMemory()\n",
    "\n",
    "    def add_to_memory(self, human_input: str, ai_response: str) -> None:\n",
    "        \"\"\"Add conversation to both short-term and long-term memory\"\"\"\n",
    "\n",
    "        # Short-term memory\n",
    "        self.short_term_memory.save_context(\n",
    "            {\"input\": human_input},\n",
    "            {\"output\": ai_response}\n",
    "        )\n",
    "        \n",
    "        # long-term memory\n",
    "        metadata = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }        \n",
    "\n",
    "        memory_text = f\"Human: {human_input}\\nAi: {ai_response}\"\n",
    "\n",
    "        self.vector_store.add_texts(\n",
    "            texts = [memory_text],\n",
    "            metadata = [metadata]\n",
    "        )\n",
    "\n",
    "    def get_relevant_memories(self, query: str) -> str:\n",
    "        \"\"\"Retrieve relevant past conversations.\"\"\"\n",
    "\n",
    "        docs = self.vector_store.similarity_search(query, k=self.memory_k)\n",
    "\n",
    "        formated_memories = []\n",
    "        for doc in docs:\n",
    "            metadata = doc.metadata\n",
    "            timestamp = metadata.get(\"timestamp\", \"Unknown time\")\n",
    "            formated_memories.append(f\"{timestamp}: {doc.page_content}\")\n",
    "\n",
    "        return \"\\n\\n\".join(formated_memories)\n",
    "\n",
    "    def get_conversation_history(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the recent conversation history.\"\"\"\n",
    "        return self.short_term_memory.load_memory_variables({})\n",
    "    \n",
    "    def clear_short_term_memory(self) -> None:\n",
    "        \"\"\"Clear the short-term memory.\"\"\"\n",
    "        self.short_term_memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af20e8b",
   "metadata": {},
   "source": [
    "## Step 4: Creating the Enhanced Chatbot\n",
    "Now let's build our main chatbot class that brings everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c073b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "import logging\n",
    "\n",
    "from langchain_google_genai import (\n",
    "    ChatGoogleGenerativeAI, \n",
    "    GoogleGenerativeAIEmbeddings\n",
    ")\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from src.utils import ChatbotConfig\n",
    "from src.memory import ChatbotMemory\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class EnhancedChatbot:\n",
    "    \"\"\"Chatbot with enhanced short-term and long-term memories.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[ChatbotConfig] = None):\n",
    "\n",
    "        self.config = config or ChatbotConfig.from_env()\n",
    "\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=self.config.model_name,\n",
    "            temperature=self.config.temperature,\n",
    "            google_api_key=self.config.google_api_key\n",
    "        )\n",
    "\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=self.config.embedding_model\n",
    "        )\n",
    "\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=\"conversation_memory\",\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=self.config.chroma_persist_dir\n",
    "        )\n",
    "\n",
    "        self.memory = ChatbotMemory(\n",
    "            vector_store=self.vector_store,\n",
    "            memory_k=self.config.memory_k\n",
    "        )\n",
    "\n",
    "        self.prompt_template = self._create_prompt_template()\n",
    "\n",
    "    def _create_prompt_template(self) -> PromptTemplate:\n",
    "        \"\"\"Create the prompt template for the chatbot.\"\"\"\n",
    "\n",
    "        template = \"\"\"You are a helpful AI assistant with memory of past converstaions.\n",
    "        \n",
    "        Relevant past conversations:\n",
    "        {relevant_memories}\n",
    "\n",
    "        Recent conversation:\n",
    "        {recent_history}\n",
    "\n",
    "        Human: {user_input}\n",
    "        AI Assistant:\n",
    "        \"\"\"\n",
    "\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"relevant_memories\", \"recent_history\", \"user_input\"],\n",
    "            template=template\n",
    "        )\n",
    "\n",
    "    def generate_response(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a response to user input.\"\"\"\n",
    "        try:\n",
    "            relevant_memories = self.memory.get_relevant_memories(user_input)\n",
    "\n",
    "            recent_history = self.memory.get_conversation_history().get('history', '')\n",
    "\n",
    "\n",
    "            prompt = self.prompt_template.format(\n",
    "                relevant_memories=relevant_memories,\n",
    "                recent_history=recent_history,\n",
    "                user_input=user_input\n",
    "            )\n",
    "\n",
    "            # Generate response\n",
    "            response = self.llm.invoke(prompt)\n",
    "            response = response.content\n",
    "\n",
    "            # Add to memory\n",
    "            self.memory.add_to_memory(user_input, response)\n",
    "\n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"relevant_memories\": relevant_memories,\n",
    "                \"success\": True\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response; {e}\")\n",
    "            return {\n",
    "                \"response\": \"I'm sorry, I encountered an error. Please try again.\",\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            }\n",
    "        \n",
    "    def clear_memory(self) -> None:\n",
    "        \"\"\"Clear the chatbot's memory.\"\"\"                      \n",
    "        self.memory.clear_short_term_memory()\n",
    "        logger.info(\"Short-term memory cleared\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e7142",
   "metadata": {},
   "source": [
    "## Interactive Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fcc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikzadbabaii/Desktop/projects/learning/LLM/chatbot_with_memory/src/memory.py:18: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.short_term_memory = ConversationBufferMemory()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Chatbot with Memory\n",
      "Type 'exit' to quit, 'clear' to clear memory\n",
      "--------------------------------------------------\n",
      "You: what is your name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8g/w0k8k7s135vc0mrp37stcbhc0000gn/T/ipykernel_15790/4236009300.py:84: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: You can call me Optimus.\n",
      "You: clear\n",
      "Memory cleared!\n",
      "You: what is your name?\n",
      "Bot: You can call me Optimus.\n",
      "You: waiting\n",
      "Bot: While you're waiting, is there anything I can help you with? Perhaps you'd like some information, a joke, or a creative writing prompt?\n"
     ]
    }
   ],
   "source": [
    "def chat_interface(chatbot: EnhancedChatbot):\n",
    "    \"\"\"Simple interactive chat interface.\"\"\"\n",
    "    print(\"Enhanced Chatbot with Memory\")\n",
    "    print(\"Type 'exit' to quit, 'clear' to clear memory\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        print(f\"You: {user_input}\")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif user_input.lower() == 'clear':\n",
    "            chatbot.clear_memory()\n",
    "            print(\"Memory cleared!\")\n",
    "            continue\n",
    "        \n",
    "        response = chatbot.generate_response(user_input)\n",
    "        print(f\"Bot: {response['response']}\")\n",
    "        \n",
    "        if not response['success']:\n",
    "            print(f\"Error: {response.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Uncomment to run the interactive interface\n",
    "config = ChatbotConfig(google_api_key=GOOGLE_API_KEY)\n",
    "chatbot = EnhancedChatbot(config)\n",
    "chat_interface(chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239e79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
